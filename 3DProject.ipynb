{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bd33866",
   "metadata": {},
   "source": [
    "## Download the dataset and check it's there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014261a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ModelNet40/ModelNet40/airplane"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004ea330",
   "metadata": {},
   "source": [
    "# Dataset Loader\n",
    "\n",
    "Load the ModelNet40 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e50a2a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import trimesh\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Dataset class\n",
    "class ModelNet40Dataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', augment=False, voxel_size=32):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.augment = augment\n",
    "        self.voxel_size = voxel_size\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.files = []\n",
    "\n",
    "        # Collect all .off files\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(root_dir, class_name, self.split)\n",
    "            for file_name in os.listdir(class_dir):\n",
    "                if file_name.endswith('.off'):\n",
    "                    self.files.append((class_name, os.path.join(class_dir, file_name)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        class_name, file_path = self.files[idx]\n",
    "        mesh = trimesh.load(file_path)\n",
    "\n",
    "        # Handle cases where mesh might not load properly\n",
    "        if not isinstance(mesh, trimesh.Trimesh):\n",
    "            print(f\"Warning: {file_path} did not load properly.\")\n",
    "            return None, None\n",
    "\n",
    "        vertices = torch.tensor(mesh.vertices, dtype=torch.float32)\n",
    "\n",
    "        # Normalize the point cloud\n",
    "        vertices = self.normalize(vertices)\n",
    "\n",
    "        # Apply augmentation if enabled\n",
    "        if self.augment:\n",
    "            vertices = self.augment_data(vertices)\n",
    "\n",
    "        # Convert to voxel grid\n",
    "        voxel_grid = self.point_cloud_to_voxel(vertices)\n",
    "\n",
    "        label = self.classes.index(class_name)\n",
    "        return voxel_grid, label\n",
    "\n",
    "    def normalize(self, vertices):\n",
    "        centroid = vertices.mean(dim=0)\n",
    "        vertices = vertices - centroid\n",
    "        scale = vertices.norm(p=2, dim=1).max()\n",
    "        vertices = vertices / scale\n",
    "        return vertices\n",
    "\n",
    "    def augment_data(self, vertices):\n",
    "        # Random rotation, scaling, and translation with reduced magnitude\n",
    "        angle = torch.tensor(random.uniform(0, 2 * torch.pi))\n",
    "        rotation_matrix = torch.tensor([\n",
    "            [torch.cos(angle), -torch.sin(angle), 0],\n",
    "            [torch.sin(angle), torch.cos(angle), 0],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "        vertices = vertices @ rotation_matrix.T\n",
    "\n",
    "        # Reduced scaling factor\n",
    "        scale_factor = random.uniform(0.95, 1.05)\n",
    "        vertices *= scale_factor\n",
    "\n",
    "        # Reduced translation\n",
    "        translation = torch.tensor([random.uniform(-0.05, 0.05) for _ in range(3)])\n",
    "        vertices += translation\n",
    "\n",
    "        return vertices\n",
    "\n",
    "    def point_cloud_to_voxel(self, vertices):\n",
    "        # Create an empty voxel grid\n",
    "        voxel_grid = np.zeros((self.voxel_size, self.voxel_size, self.voxel_size), dtype=np.float32)\n",
    "        \n",
    "        # Scale and shift points to fit in the voxel grid\n",
    "        vertices = vertices.numpy()  # Convert to NumPy array for processing\n",
    "        vertices = ((vertices + 1) / 2) * (self.voxel_size - 1)\n",
    "        vertices = np.clip(vertices, 0, self.voxel_size - 1)  # Clamp indices to valid range\n",
    "        vertices = vertices.astype(np.int32)\n",
    "        \n",
    "        # Mark the occupied voxels\n",
    "        voxel_grid[vertices[:, 0], vertices[:, 1], vertices[:, 2]] = 1.0\n",
    "        \n",
    "        # Convert to a PyTorch tensor and add a channel dimension (required for Conv3D)\n",
    "        voxel_grid = torch.tensor(voxel_grid).unsqueeze(0)\n",
    "        \n",
    "        return voxel_grid\n",
    "\n",
    "def collate(batch):\n",
    "    voxel_grids, labels = zip(*batch)\n",
    "    voxel_grids = torch.stack(voxel_grids, dim=0)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return voxel_grids, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ef01af",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "## 3D CNN model used the VoxelNet architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0612f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voxel-based 3D CNN model with Adaptive Pooling\n",
    "class VoxelNet(nn.Module):\n",
    "    def __init__(self, num_classes=40):\n",
    "        super(VoxelNet, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(32)\n",
    "        self.conv2 = nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(64)\n",
    "        self.conv3 = nn.Conv3d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm3d(128)\n",
    "        self.conv4 = nn.Conv3d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm3d(256)\n",
    "        \n",
    "        # Adaptive Max Pooling to ensure fixed output size regardless of input size\n",
    "        self.pool = nn.AdaptiveMaxPool3d((4, 4, 4))\n",
    "        \n",
    "        # Now we know the flattened size will be 256 * 4 * 4 * 4\n",
    "        self.fc1 = nn.Linear(256 * 4 * 4 * 4, 512)\n",
    "        self.bn5 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn6 = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Flatten the output before feeding into fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.bn5(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn6(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b50247",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation (no patches)\n",
    "\n",
    "Define training and evaluating methods to train the model without a patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f63c713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, criterion, device, max_batches=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (voxels, labels) in enumerate(dataloader):\n",
    "        if max_batches and batch_idx >= max_batches:\n",
    "            break\n",
    "\n",
    "        voxels = voxels.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(voxels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device, max_batches=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (voxels, labels) in enumerate(dataloader):\n",
    "            if max_batches and batch_idx >= max_batches:\n",
    "                break\n",
    "\n",
    "            voxels = voxels.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(voxels)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "adc6c30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is running on the cpu environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch3d/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Main script remains the same\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"The model is running on the {device} environment\")\n",
    "\n",
    "# Initialize the Enhanced VoxelNet model\n",
    "model = VoxelNet(num_classes=40).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "train_dataset = ModelNet40Dataset(root_dir='ModelNet40/ModelNet40', augment=True, voxel_size=32)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate, num_workers=0)\n",
    "\n",
    "val_dataset = ModelNet40Dataset(root_dir='ModelNet40/ModelNet40', split='test', voxel_size=32)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e546be8",
   "metadata": {},
   "source": [
    "## Main Training Loop (no patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e263cc8e-93e4-45db-8fa2-2bea343c7ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is running on the cpu environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch3d/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 is running...\n",
      "Epoch [1/10], Train Loss: 0.2198, Train Accuracy: 17.19%, Val Loss: 0.4724, Val Accuracy: 15.31%\n",
      "Epoch 2 is running...\n",
      "Epoch [2/10], Train Loss: 0.1951, Train Accuracy: 29.53%, Val Loss: 0.3346, Val Accuracy: 42.81%\n",
      "Epoch 3 is running...\n",
      "Epoch [3/10], Train Loss: 0.1852, Train Accuracy: 31.41%, Val Loss: 0.3173, Val Accuracy: 43.44%\n",
      "Epoch 4 is running...\n",
      "Epoch [4/10], Train Loss: 0.1677, Train Accuracy: 38.28%, Val Loss: 0.2529, Val Accuracy: 50.62%\n",
      "Epoch 5 is running...\n",
      "Epoch [5/10], Train Loss: 0.1614, Train Accuracy: 39.38%, Val Loss: 0.2478, Val Accuracy: 51.25%\n",
      "Epoch 6 is running...\n",
      "Epoch [6/10], Train Loss: 0.1609, Train Accuracy: 37.50%, Val Loss: 0.2304, Val Accuracy: 57.81%\n",
      "Epoch 7 is running...\n",
      "Epoch [7/10], Train Loss: 0.1533, Train Accuracy: 40.94%, Val Loss: 0.2242, Val Accuracy: 55.94%\n",
      "Epoch 8 is running...\n",
      "Epoch [8/10], Train Loss: 0.1502, Train Accuracy: 41.56%, Val Loss: 0.2153, Val Accuracy: 61.56%\n",
      "Epoch 9 is running...\n",
      "Epoch [9/10], Train Loss: 0.1449, Train Accuracy: 45.16%, Val Loss: 0.2468, Val Accuracy: 53.12%\n",
      "Epoch 10 is running...\n",
      "Epoch [10/10], Train Loss: 0.1471, Train Accuracy: 41.09%, Val Loss: 0.2321, Val Accuracy: 57.81%\n",
      "Test Loss: 0.4724, Test Accuracy: 61.56%\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = 0\n",
    "early_stop_counter = 0\n",
    "early_stop_patience = 10\n",
    "no_patch_test_loss, no_patch_test_acc = 1, 0\n",
    "no_patch_cumulative_losses = []\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1} is running...\")\n",
    "    train_loss, train_acc = train_model(model, train_dataloader, optimizer, criterion, device, max_batches=40)\n",
    "    val_loss, val_acc = evaluate_model(model, val_dataloader, criterion, device, max_batches=20)\n",
    "    \n",
    "    no_patch_cumulative_losses.append(val_loss)\n",
    "    no_patch_test_loss = min(no_patch_test_loss, val_loss)\n",
    "    no_patch_test_acc = max(no_patch_test_acc, val_acc)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= early_stop_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(f'Test Loss: {no_patch_test_loss:.4f}, Test Accuracy: {no_patch_test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41436f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is because I forgot to define these attributes when I was running the model\n",
    "# It takes way too long to run so I manually added the loss values \n",
    "no_patch_cumulative_losses = [0.4724, 0.3346, 0.3173, 0.2529, 0.2478, 0.2304, 0.2242, 0.2153, 0.2468, 0.2321]\n",
    "no_patch_test_loss = 0.2153\n",
    "no_patch_test_acc = 61.56"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b6584f",
   "metadata": {},
   "source": [
    "# Adverserial Attacks\n",
    "\n",
    "## Gaussian Attack Patch\n",
    "Define the main gaussian attack adverserial attack method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e16ea793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_attack(voxel_grids, mean=0.0, stddev=0.01):\n",
    "    \"\"\"\n",
    "    Apply Gaussian noise to the voxel grids.\n",
    "\n",
    "    Args:\n",
    "    voxel_grids (Tensor): A batch of voxel grids (B, C, H, W, D).\n",
    "    mean (float): Mean of the Gaussian noise.\n",
    "    stddev (float): Standard deviation of the Gaussian noise.\n",
    "\n",
    "    Returns:\n",
    "    Tensor: The perturbed voxel grids.\n",
    "    \"\"\"\n",
    "    # Generate Gaussian noise\n",
    "    noise = torch.normal(mean, stddev, size=voxel_grids.size(), device=voxel_grids.device)\n",
    "\n",
    "    # Apply the noise to the voxel grids\n",
    "    perturbed_voxel_grids = voxel_grids + noise\n",
    "\n",
    "    # Ensure the voxel values remain in a valid range (e.g., [0, 1] if binary)\n",
    "    perturbed_voxel_grids = torch.clamp(perturbed_voxel_grids, 0, 1)\n",
    "\n",
    "    return perturbed_voxel_grids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cbe332",
   "metadata": {},
   "source": [
    "Training and Evaluation methods for GA Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "824a22d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_gaussian_attack(model, dataloader, optimizer, criterion, device, mean=0.0, stddev=0.01, max_batches=None):\n",
    "    \"\"\"\n",
    "    Train the model with Gaussian noise added to the voxel grids.\n",
    "\n",
    "    Args:\n",
    "    model (nn.Module): The neural network model.\n",
    "    dataloader (DataLoader): DataLoader for the training data.\n",
    "    optimizer (Optimizer): Optimizer for training.\n",
    "    criterion (Loss): Loss function.\n",
    "    device (torch.device): Device to run the training on.\n",
    "    mean (float): Mean of the Gaussian noise.\n",
    "    stddev (float): Standard deviation of the Gaussian noise.\n",
    "    max_batches (int): Maximum number of batches to process in each epoch (for debugging).\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (voxels, labels) in enumerate(dataloader):\n",
    "        if max_batches and batch_idx >= max_batches:\n",
    "            break\n",
    "        voxels = voxels.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "\n",
    "        # Apply Gaussian attack to the voxel grids\n",
    "        perturbed_voxels = gaussian_attack(voxels, mean, stddev)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(perturbed_voxels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss and accuracy\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate_with_gaussian_attack(model, dataloader, criterion, device, mean=0.0, stddev=0.01, max_batches=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (voxels, labels) in enumerate(dataloader):\n",
    "            if max_batches and batch_idx >= max_batches:\n",
    "                break\n",
    "            voxels = voxels.to(device)\n",
    "            labels = labels.to(device).long()\n",
    "\n",
    "            # Apply Gaussian attack\n",
    "            perturbed_voxels = gaussian_attack(voxels, mean, stddev)\n",
    "\n",
    "            # Forward pass with perturbed voxel grids\n",
    "            outputs = model(perturbed_voxels)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edf4e70",
   "metadata": {},
   "source": [
    "## Main training loop for GA patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93416012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 is running...\n",
      "Epoch [1/10], Train Loss: 0.1377, Train Accuracy: 45.31%, Val Loss: 0.2190, Val Accuracy: 59.69%\n",
      "Epoch 2 is running...\n",
      "Epoch [2/10], Train Loss: 0.1329, Train Accuracy: 47.81%, Val Loss: 0.2058, Val Accuracy: 60.00%\n",
      "Epoch 3 is running...\n",
      "Epoch [3/10], Train Loss: 0.1355, Train Accuracy: 46.72%, Val Loss: 0.2157, Val Accuracy: 59.38%\n",
      "Epoch 4 is running...\n",
      "Epoch [4/10], Train Loss: 0.1278, Train Accuracy: 51.56%, Val Loss: 0.2093, Val Accuracy: 59.38%\n",
      "Epoch 5 is running...\n",
      "Epoch [5/10], Train Loss: 0.1219, Train Accuracy: 53.44%, Val Loss: 0.1952, Val Accuracy: 62.81%\n",
      "Epoch 6 is running...\n",
      "Epoch [6/10], Train Loss: 0.1298, Train Accuracy: 48.12%, Val Loss: 0.1953, Val Accuracy: 64.38%\n",
      "Epoch 7 is running...\n",
      "Epoch [7/10], Train Loss: 0.1262, Train Accuracy: 50.00%, Val Loss: 0.1862, Val Accuracy: 65.31%\n",
      "Epoch 8 is running...\n",
      "Epoch [8/10], Train Loss: 0.1184, Train Accuracy: 54.06%, Val Loss: 0.1897, Val Accuracy: 64.38%\n",
      "Epoch 9 is running...\n",
      "Epoch [9/10], Train Loss: 0.1204, Train Accuracy: 51.41%, Val Loss: 0.1758, Val Accuracy: 65.62%\n",
      "Epoch 10 is running...\n",
      "Epoch [10/10], Train Loss: 0.1207, Train Accuracy: 53.59%, Val Loss: 0.1889, Val Accuracy: 62.19%\n",
      "Test Loss: 0.2190, Test Accuracy: 65.62%\n"
     ]
    }
   ],
   "source": [
    "# Main training loop with Gaussian attack\n",
    "gaussian_mean = 0.0\n",
    "gaussian_stddev = 0.02  # Adjust the standard deviation to control the strength of the attack\n",
    "ga_test_loss, ga_test_acc = 1, 0\n",
    "ga_cumulative_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1} is running...\")\n",
    "    train_loss, train_acc = train_model_with_gaussian_attack(model, train_dataloader, optimizer, criterion, device, mean=gaussian_mean, stddev=gaussian_stddev, max_batches=40)\n",
    "    val_loss, val_acc = evaluate_with_gaussian_attack(model, val_dataloader, criterion, device, mean=gaussian_mean, stddev=gaussian_stddev, max_batches=20)\n",
    "\n",
    "    ga_cumulative_loss.append(val_loss)\n",
    "    ga_test_loss = min(ga_test_loss, val_loss)\n",
    "    ga_test_acc = max(ga_test_acc, val_acc)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    # Pass the validation loss to the scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "print(f'Test Loss: {ga_test_loss:.4f}, Test Accuracy: {ga_test_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dbb79da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is because I forgot to define these attributes when I was running the model\n",
    "# It takes way too long to run so I manually added the loss values \n",
    "ga_cumulative_loss = [0.2190, 0.2058, 0.2157, 0.2093, 0.1952, 0.1953, 0.1862, 0.1897, 0.1758, 0.1889]\n",
    "ga_test_loss = 0.1758\n",
    "ga_test_acc = 65.62"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb9fd37",
   "metadata": {},
   "source": [
    "## FGSM Attack Patch\n",
    "Define main FGSM adverserial attack method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c3700306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(voxel_grids, labels, model, criterion, epsilon=0.01):\n",
    "    \"\"\"\n",
    "    Perform FGSM attack on the voxel grids.\n",
    "\n",
    "    Args:\n",
    "    voxel_grids (Tensor): A batch of voxel grids (B, C, H, W, D).\n",
    "    labels (Tensor): Ground truth labels for the voxel grids.\n",
    "    model (nn.Module): The neural network model.\n",
    "    criterion (nn.Module): The loss function used for the attack.\n",
    "    epsilon (float): The perturbation magnitude.\n",
    "\n",
    "    Returns:\n",
    "    Tensor: The perturbed voxel grids.\n",
    "    \"\"\"\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Make the voxel grids require gradients\n",
    "    voxel_grids.requires_grad = True\n",
    "\n",
    "    # Forward pass through the model\n",
    "    outputs = model(voxel_grids)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Backward pass to calculate gradients\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # FGSM perturbation: sign of the gradient\n",
    "    perturbation = epsilon * voxel_grids.grad.sign()\n",
    "\n",
    "    # Apply the perturbation to the voxel grids\n",
    "    perturbed_voxel_grids = voxel_grids + perturbation\n",
    "\n",
    "    # Ensure voxel grid values remain in a valid range (e.g., [0, 1])\n",
    "    perturbed_voxel_grids = torch.clamp(perturbed_voxel_grids, 0, 1)\n",
    "\n",
    "    return perturbed_voxel_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f5ca91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_fgsm_attack(model, dataloader, optimizer, criterion, device, epsilon=0.01, max_batches=None):\n",
    "    \"\"\"\n",
    "    Train the model with FGSM attack applied to the voxel grids.\n",
    "\n",
    "    Args:\n",
    "    model (nn.Module): The neural network model.\n",
    "    dataloader (DataLoader): DataLoader for the training data.\n",
    "    optimizer (Optimizer): Optimizer for training.\n",
    "    criterion (Loss): Loss function.\n",
    "    device (torch.device): Device to run the training on.\n",
    "    epsilon (float): The perturbation magnitude for FGSM.\n",
    "    max_batches (int): Maximum number of batches to process in each epoch (for debugging).\n",
    "\n",
    "    Returns:\n",
    "    avg_loss (float): Average loss over the training epoch.\n",
    "    accuracy (float): Accuracy of the model on the training data.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (voxels, labels) in enumerate(dataloader):\n",
    "        if max_batches and batch_idx >= max_batches:\n",
    "            break\n",
    "        voxels = voxels.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "\n",
    "        # Apply FGSM attack to the voxel grids\n",
    "        perturbed_voxels = fgsm_attack(voxels, labels, model, criterion, epsilon)\n",
    "\n",
    "        # Forward pass with perturbed voxel grids\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(perturbed_voxels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss and accuracy\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate_with_fgsm_attack(model, dataloader, criterion, device, epsilon=0.01, max_batches=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (voxels, labels) in enumerate(dataloader):\n",
    "        if max_batches and batch_idx >= max_batches:\n",
    "            break\n",
    "        voxels = voxels.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "\n",
    "        # Apply FGSM attack\n",
    "        perturbed_voxels = fgsm_attack(voxels, labels, model, criterion, epsilon)\n",
    "\n",
    "        # Forward pass with perturbed voxel grids\n",
    "        outputs = model(perturbed_voxels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af34de61",
   "metadata": {},
   "source": [
    "## Main training loop for FGSM patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdfd267a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 is running...\n",
      "Epoch [1/10], Train Loss: 0.1271, Train Accuracy: 45.47%, Val Loss: 0.2113, Val Accuracy: 55.94%\n",
      "Epoch 2 is running...\n",
      "Epoch [2/10], Train Loss: 0.1173, Train Accuracy: 48.28%, Val Loss: 0.2135, Val Accuracy: 56.25%\n",
      "Epoch 3 is running...\n",
      "Epoch [3/10], Train Loss: 0.1052, Train Accuracy: 52.66%, Val Loss: 0.1839, Val Accuracy: 60.31%\n",
      "Epoch 4 is running...\n",
      "Epoch [4/10], Train Loss: 0.1051, Train Accuracy: 53.59%, Val Loss: 0.1835, Val Accuracy: 60.00%\n",
      "Epoch 5 is running...\n",
      "Epoch [5/10], Train Loss: 0.0994, Train Accuracy: 54.84%, Val Loss: 0.1845, Val Accuracy: 59.69%\n",
      "Epoch 6 is running...\n",
      "Epoch [6/10], Train Loss: 0.0973, Train Accuracy: 58.59%, Val Loss: 0.1906, Val Accuracy: 58.44%\n",
      "Epoch 7 is running...\n",
      "Epoch [7/10], Train Loss: 0.0931, Train Accuracy: 57.19%, Val Loss: 0.2078, Val Accuracy: 57.19%\n",
      "Epoch 8 is running...\n",
      "Epoch [8/10], Train Loss: 0.0928, Train Accuracy: 58.75%, Val Loss: 0.2049, Val Accuracy: 58.75%\n",
      "Epoch 9 is running...\n",
      "Epoch [9/10], Train Loss: 0.0910, Train Accuracy: 58.75%, Val Loss: 0.1905, Val Accuracy: 60.62%\n",
      "Epoch 10 is running...\n",
      "Epoch [10/10], Train Loss: 0.0959, Train Accuracy: 55.16%, Val Loss: 0.1841, Val Accuracy: 60.94%\n",
      "FGSM Attack - Test Loss: 0.2135, Test Accuracy: 60.94%\n"
     ]
    }
   ],
   "source": [
    "# Main training loop with FGSM attack\n",
    "fgsm_epsilon = 0.01  # Adjust the epsilon value to control the strength of the attack\n",
    "fgsm_test_loss, fgsm_test_acc = 1, 0\n",
    "fgsm_cumulative_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1} is running...\")\n",
    "    train_loss, train_acc = train_model_with_fgsm_attack(model, train_dataloader, optimizer, criterion, device, epsilon=fgsm_epsilon, max_batches=40)\n",
    "    val_loss, val_acc = evaluate_with_fgsm_attack(model, val_dataloader, criterion, device, epsilon=fgsm_epsilon, max_batches=20)\n",
    "\n",
    "    fgsm_cumulative_loss.append(val_loss)\n",
    "    fgsm_test_loss = min(fgsm_test_loss, val_loss)\n",
    "    fgsm_test_acc = max(fgsm_test_acc, val_acc)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    # Update the learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "print(f'FGSM Attack - Test Loss: {fgsm_test_loss:.4f}, Test Accuracy: {fgsm_test_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21396950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is because I forgot to define these attributes when I was running the model\n",
    "# It takes way too long to run so I manually added the loss values \n",
    "fgsm_cumulative_loss = [0.2113, 0.2135, 0.1839, 0.1835, 0.1845, 0.1906, 0.2078, 0.2049, 0.1905, 0.1841]\n",
    "fgsm_test_loss = 0.1835\n",
    "fgsm_test_acc = 60.94"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e314d3d5",
   "metadata": {},
   "source": [
    "## PGD Attack Patch\n",
    "Define the main PGD adverserial attack method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b38693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_attack(voxel_grids, labels, model, criterion, epsilon=0.01, alpha=0.001, num_iter=40):\n",
    "    \"\"\"\n",
    "    Perform PGD attack on the voxel grids.\n",
    "\n",
    "    Args:\n",
    "    voxel_grids (Tensor): A batch of voxel grids (B, C, H, W, D).\n",
    "    labels (Tensor): Ground truth labels for the voxel grids.\n",
    "    model (nn.Module): The neural network model.\n",
    "    criterion (nn.Module): The loss function used for the attack.\n",
    "    epsilon (float): Maximum perturbation magnitude.\n",
    "    alpha (float): Step size for each iteration.\n",
    "    num_iter (int): Number of iterations for the attack.\n",
    "\n",
    "    Returns:\n",
    "    Tensor: The perturbed voxel grids.\n",
    "    \"\"\"\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Clone the voxel grids for perturbation\n",
    "    original_voxel_grids = voxel_grids.clone().detach()\n",
    "    perturbed_voxel_grids = original_voxel_grids.clone().detach().requires_grad_(True)\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        # Forward pass through the model\n",
    "        outputs = model(perturbed_voxel_grids)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass to calculate gradients\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # PGD step: apply FGSM-like perturbation\n",
    "        perturbation = alpha * perturbed_voxel_grids.grad.sign()\n",
    "\n",
    "        # Update the perturbed voxel grids\n",
    "        perturbed_voxel_grids = perturbed_voxel_grids.detach() + perturbation\n",
    "\n",
    "        # Project the perturbation to the epsilon ball\n",
    "        perturbation = torch.clamp(perturbed_voxel_grids - original_voxel_grids, -epsilon, epsilon)\n",
    "        perturbed_voxel_grids = (original_voxel_grids + perturbation).clamp(0, 1).requires_grad_(True)\n",
    "\n",
    "    return perturbed_voxel_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a93ad5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_pgd_attack(model, dataloader, optimizer, criterion, device, epsilon=0.01, alpha=0.001, num_iter=40, max_batches=None):\n",
    "    \"\"\"\n",
    "    Train the model with PGD attack applied to the voxel grids.\n",
    "\n",
    "    Args:\n",
    "    model (nn.Module): The neural network model.\n",
    "    dataloader (DataLoader): DataLoader for the training data.\n",
    "    optimizer (Optimizer): Optimizer for training.\n",
    "    criterion (Loss): Loss function.\n",
    "    device (torch.device): Device to run the training on.\n",
    "    epsilon (float): Maximum perturbation magnitude for PGD.\n",
    "    alpha (float): Step size for each iteration in PGD.\n",
    "    num_iter (int): Number of iterations for the PGD attack.\n",
    "    max_batches (int): Maximum number of batches to process in each epoch (for debugging).\n",
    "\n",
    "    Returns:\n",
    "    avg_loss (float): Average loss over the training epoch.\n",
    "    accuracy (float): Accuracy of the model on the training data.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (voxels, labels) in enumerate(dataloader):\n",
    "        if max_batches and batch_idx >= max_batches:\n",
    "            break\n",
    "        voxels = voxels.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "\n",
    "        # Apply PGD attack to the voxel grids\n",
    "        perturbed_voxels = pgd_attack(voxels, labels, model, criterion, epsilon, alpha, num_iter)\n",
    "\n",
    "        # Forward pass with perturbed voxel grids\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(perturbed_voxels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss and accuracy\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate_with_pgd_attack(model, dataloader, criterion, device, epsilon=0.01, alpha=0.001, num_iter=40, max_batches=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (voxels, labels) in enumerate(dataloader):\n",
    "        if max_batches and batch_idx >= max_batches:\n",
    "            break\n",
    "        voxels = voxels.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "\n",
    "        # Apply PGD attack\n",
    "        perturbed_voxels = pgd_attack(voxels, labels, model, criterion, epsilon, alpha, num_iter)\n",
    "\n",
    "        # Forward pass with perturbed voxel grids\n",
    "        outputs = model(perturbed_voxels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50831c9",
   "metadata": {},
   "source": [
    "## Main training loop for PGD patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dd4b723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 is running...\n",
      "Epoch [1/10], Train Loss: 0.0972, Train Accuracy: 56.41%, Val Loss: 0.1969, Val Accuracy: 59.06%\n",
      "Epoch 2 is running...\n",
      "Epoch [2/10], Train Loss: 0.0893, Train Accuracy: 58.44%, Val Loss: 0.1890, Val Accuracy: 58.44%\n",
      "Epoch 3 is running...\n",
      "Epoch [3/10], Train Loss: 0.0954, Train Accuracy: 56.88%, Val Loss: 0.1869, Val Accuracy: 59.38%\n",
      "Epoch 4 is running...\n",
      "Epoch [4/10], Train Loss: 0.0855, Train Accuracy: 60.94%, Val Loss: 0.1879, Val Accuracy: 59.38%\n",
      "Epoch 5 is running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch3d/lib/python3.12/site-packages/trimesh/grouping.py:99: RuntimeWarning: invalid value encountered in cast\n",
      "  stacked = np.column_stack(stacked).round().astype(np.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Train Loss: 0.0909, Train Accuracy: 59.06%, Val Loss: 0.1867, Val Accuracy: 58.75%\n",
      "Epoch 6 is running...\n",
      "Epoch [6/10], Train Loss: 0.0983, Train Accuracy: 56.88%, Val Loss: 0.1870, Val Accuracy: 59.69%\n",
      "Epoch 7 is running...\n",
      "Epoch [7/10], Train Loss: 0.0914, Train Accuracy: 58.91%, Val Loss: 0.1857, Val Accuracy: 59.38%\n",
      "Epoch 8 is running...\n",
      "Epoch [8/10], Train Loss: 0.0929, Train Accuracy: 58.12%, Val Loss: 0.1885, Val Accuracy: 58.75%\n",
      "Epoch 9 is running...\n",
      "Epoch [9/10], Train Loss: 0.0928, Train Accuracy: 58.12%, Val Loss: 0.1888, Val Accuracy: 59.06%\n",
      "Epoch 10 is running...\n",
      "Epoch [10/10], Train Loss: 0.0963, Train Accuracy: 58.75%, Val Loss: 0.1881, Val Accuracy: 58.75%\n",
      "PGD Attack - Test Loss: 0.1969, Test Accuracy: 59.69%\n"
     ]
    }
   ],
   "source": [
    "# Main training loop with PGD attack\n",
    "pgd_epsilon = 0.01  # Maximum perturbation\n",
    "pgd_alpha = 0.001   # Step size\n",
    "pgd_num_iter = 10   # Number of iterations\n",
    "pgd_test_loss, pgd_test_acc = 1, 0\n",
    "pgd_cumulative_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1} is running...\")\n",
    "    train_loss, train_acc = train_model_with_pgd_attack(model, train_dataloader, optimizer, criterion, device, epsilon=pgd_epsilon, alpha=pgd_alpha, num_iter=pgd_num_iter, max_batches=40)\n",
    "    val_loss, val_acc = evaluate_with_pgd_attack(model, val_dataloader, criterion, device, epsilon=pgd_epsilon, alpha=pgd_alpha, num_iter=pgd_num_iter, max_batches=20)\n",
    "\n",
    "    pgd_cumulative_loss.append(val_loss)\n",
    "    pgd_test_loss = min(pgd_test_loss, val_loss) \n",
    "    pgd_test_acc = max(pgd_test_acc, val_acc)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    # Update the learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "print(f'PGD Attack - Test Loss: {pgd_test_loss:.4f}, Test Accuracy: {pgd_test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68597d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is because I forgot to define these attributes when I was running the model\n",
    "# It takes way too long to run so I manually added the loss values \n",
    "pgd_cumulative_loss = [0.1969, 0.1890, 0.1869, 0.1879, 0.1867, 0.1870, 0.1857, 0.1885, 0.1888, 0.1881]\n",
    "pgd_test_loss = 0.1857\n",
    "pgd_test_acc = 59.69"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698bc49a",
   "metadata": {},
   "source": [
    "## MIM Attack Patch\n",
    "Define the main MIM adverserial attack method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb75b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mim_attack(voxel_grids, labels, model, criterion, epsilon=0.01, alpha=0.001, num_iter=40, decay_factor=1.0):\n",
    "    \"\"\"\n",
    "    Perform Momentum Iterative Method (MIM) attack on the voxel grids.\n",
    "\n",
    "    Args:\n",
    "    voxel_grids (Tensor): A batch of voxel grids (B, C, H, W, D).\n",
    "    labels (Tensor): Ground truth labels for the voxel grids.\n",
    "    model (nn.Module): The neural network model.\n",
    "    criterion (nn.Module): The loss function used for the attack.\n",
    "    epsilon (float): Maximum perturbation magnitude.\n",
    "    alpha (float): Step size for each iteration.\n",
    "    num_iter (int): Number of iterations for the attack.\n",
    "    decay_factor (float): Decay factor for the momentum term.\n",
    "\n",
    "    Returns:\n",
    "    Tensor: The perturbed voxel grids.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Clone the voxel grids for perturbation\n",
    "    perturbed_voxel_grids = voxel_grids.clone().detach().to(device)\n",
    "    g = torch.zeros_like(perturbed_voxel_grids).to(device)  # Initialize momentum term\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        perturbed_voxel_grids.requires_grad = True  # Ensure gradients are tracked\n",
    "        outputs = model(perturbed_voxel_grids)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass to calculate gradients\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Accumulate momentum\n",
    "        grad = perturbed_voxel_grids.grad.data\n",
    "        g = decay_factor * g + grad / grad.abs().mean(dim=(1,2,3,4), keepdim=True)\n",
    "\n",
    "        # Apply perturbation\n",
    "        perturbation = alpha * g.sign()\n",
    "        perturbed_voxel_grids = perturbed_voxel_grids + perturbation\n",
    "\n",
    "        # Project the perturbation to the epsilon ball\n",
    "        perturbed_voxel_grids = torch.clamp(perturbed_voxel_grids, voxel_grids - epsilon, voxel_grids + epsilon)\n",
    "        perturbed_voxel_grids = torch.clamp(perturbed_voxel_grids, 0, 1).detach()\n",
    "\n",
    "    return perturbed_voxel_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c064716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_mim_attack(model, dataloader, optimizer, criterion, device, epsilon=0.01, alpha=0.001, num_iter=40, decay_factor=1.0, max_batches=None):\n",
    "    \"\"\"\n",
    "    Train the model with MIM attack applied to the voxel grids.\n",
    "\n",
    "    Args:\n",
    "    model (nn.Module): The neural network model.\n",
    "    dataloader (DataLoader): DataLoader for the training data.\n",
    "    optimizer (Optimizer): Optimizer for training.\n",
    "    criterion (Loss): Loss function.\n",
    "    device (torch.device): Device to run the training on.\n",
    "    epsilon (float): Maximum perturbation magnitude for MIM.\n",
    "    alpha (float): Step size for each iteration in MIM.\n",
    "    num_iter (int): Number of iterations for the MIM attack.\n",
    "    decay_factor (float): Decay factor for the momentum term.\n",
    "    max_batches (int): Maximum number of batches to process in each epoch (for debugging).\n",
    "\n",
    "    Returns:\n",
    "    avg_loss (float): Average loss over the training epoch.\n",
    "    accuracy (float): Accuracy of the model on the training data.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (voxels, labels) in enumerate(dataloader):\n",
    "        if max_batches and batch_idx >= max_batches:\n",
    "            break\n",
    "        voxels = voxels.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "\n",
    "        # Apply MIM attack to the voxel grids\n",
    "        perturbed_voxels = mim_attack(voxels, labels, model, criterion, epsilon, alpha, num_iter, decay_factor)\n",
    "\n",
    "        # Forward pass with perturbed voxel grids\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(perturbed_voxels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss and accuracy\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate_with_mim_attack(model, dataloader, criterion, device, epsilon=0.01, alpha=0.001, num_iter=40, decay_factor=1.0, max_batches=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (voxels, labels) in enumerate(dataloader):\n",
    "        if max_batches and batch_idx >= max_batches:\n",
    "            break\n",
    "        voxels = voxels.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "\n",
    "        # Apply MIM attack\n",
    "        perturbed_voxels = mim_attack(voxels, labels, model, criterion, epsilon, alpha, num_iter, decay_factor)\n",
    "\n",
    "        # Forward pass with perturbed voxel grids\n",
    "        outputs = model(perturbed_voxels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045b7db3",
   "metadata": {},
   "source": [
    "## The main training loop for the MIM patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c56e140c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 is running...\n",
      "Epoch [1/10], Train Loss: 0.1310, Train Accuracy: 28.44%, Val Loss: 0.2260, Val Accuracy: 40.22%\n",
      "Epoch 2 is running...\n",
      "Epoch [2/10], Train Loss: 0.1322, Train Accuracy: 22.35%, Val Loss: 0.2060, Val Accuracy: 51.19%\n",
      "Epoch 3 is running...\n",
      "Epoch [3/10], Train Loss: 0.1251, Train Accuracy: 32.50%, Val Loss: 0.2183, Val Accuracy: 20.31%\n",
      "Epoch 4 is running...\n",
      "Epoch [4/10], Train Loss: 0.1276, Train Accuracy: 30.26%, Val Loss: 0.2235, Val Accuracy: 11.38%\n",
      "Epoch 5 is running...\n",
      "Epoch [5/10], Train Loss: 0.1266, Train Accuracy: 29.04%, Val Loss: 0.2060, Val Accuracy: 54.03%\n",
      "Epoch 6 is running...\n",
      "Epoch [6/10], Train Loss: 0.1259, Train Accuracy: 31.28%, Val Loss: 0.2094, Val Accuracy: 36.15%\n",
      "Epoch 7 is running...\n",
      "Epoch [7/10], Train Loss: 0.1182, Train Accuracy: 37.99%, Val Loss: 0.1917, Val Accuracy: 51.19%\n",
      "Epoch 8 is running...\n",
      "Epoch [8/10], Train Loss: 0.1194, Train Accuracy: 33.92%, Val Loss: 0.1922, Val Accuracy: 63.38%\n",
      "Epoch 9 is running...\n",
      "Epoch [9/10], Train Loss: 0.1168, Train Accuracy: 36.36%, Val Loss: 0.1980, Val Accuracy: 60.12%\n",
      "Epoch 10 is running...\n",
      "Epoch [10/10], Train Loss: 0.1140, Train Accuracy: 40.42%, Val Loss: 0.1814, Val Accuracy: 66.22%\n",
      "MIM Attack - Test Loss: 0.1814, Test Accuracy: 66.22%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Main training loop with MIM attack\n",
    "num_epochs = 10\n",
    "mim_epsilon = 0.01  # Maximum perturbation\n",
    "mim_alpha = 0.001   # Step size\n",
    "mim_num_iter = 10   # Number of iterations\n",
    "decay_factor = 1.0  # Momentum decay factor\n",
    "mim_test_loss, mim_test_acc = 1, 0\n",
    "mim_cumulative_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1} is running...\")\n",
    "    train_loss, train_acc = train_model_with_mim_attack(model, train_dataloader, optimizer, criterion, device, epsilon=mim_epsilon, alpha=mim_alpha, num_iter=mim_num_iter, decay_factor=decay_factor, max_batches=40)\n",
    "    val_loss, val_acc = evaluate_with_mim_attack(model, val_dataloader, criterion, device, epsilon=mim_epsilon, alpha=mim_alpha, num_iter=mim_num_iter, decay_factor=decay_factor, max_batches=20)\n",
    "\n",
    "    mim_cumulative_loss.append(val_loss)\n",
    "    mim_test_loss = min(mim_test_loss, val_loss)\n",
    "    mim_test_acc = max(mim_test_acc, val_acc)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    # Update the learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "print(f'MIM Attack - Test Loss: {mim_test_loss:.4f}, Test Accuracy: {mim_test_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2bf4090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mim_cumulative_loss = [0.2260, 0.2060, 0.2183, 0.2235, 0.2060, 0.2094, 0.1917, 0.1922, 0.1980, 0.1814]\n",
    "mim_test_loss = 0.1814\n",
    "mim_test_acc = 66.22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b45bd21",
   "metadata": {},
   "source": [
    "# Plot results from Patch Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31474de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "clean_test_acc = no_patch_test_acc\n",
    "gaussian_test_acc = ga_test_acc\n",
    "fgsm_test_acc = fgsm_test_acc\n",
    "pgd_test_acc = pgd_test_acc\n",
    "mim_test_acc = mim_test_acc  # Include the MIM test accuracy\n",
    "\n",
    "# Gather the results\n",
    "attack_names = ['None', 'Gaussian', 'FGSM', 'PGD', 'MIM']\n",
    "accuracies = [clean_test_acc, gaussian_test_acc, fgsm_test_acc, pgd_test_acc, mim_test_acc]\n",
    "\n",
    "# Calculate relative improvements compared to clean accuracy\n",
    "relative_improvements = [(acc - clean_test_acc) / clean_test_acc * 100 for acc in accuracies]\n",
    "\n",
    "# Plotting the accuracy comparison\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Plot for Model Accuracy under Different Attacks\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(attack_names, accuracies, color=['blue', 'green', 'red', 'purple', 'orange'])\n",
    "plt.title('Model Accuracy under Different Attacks')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "# Plot for Relative Improvement Compared to Clean Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(attack_names, relative_improvements, color=['blue', 'green', 'red', 'purple', 'orange'])\n",
    "plt.title('Relative Improvement Compared to Clean Accuracy')\n",
    "plt.ylabel('Relative Improvement (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1121544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "clean_test_acc = no_patch_test_acc\n",
    "gaussian_test_acc = ga_test_acc\n",
    "fgsm_test_acc = fgsm_test_acc\n",
    "pgd_test_acc = pgd_test_acc\n",
    "mim_test_acc = mim_test_acc\n",
    "\n",
    "# Gather the results\n",
    "attack_names = ['Clean', 'Gaussian', 'FGSM', 'PGD', 'MIM']\n",
    "accuracies = [clean_test_acc, gaussian_test_acc, fgsm_test_acc, pgd_test_acc, mim_test_acc]\n",
    "\n",
    "# Calculate relative improvements\n",
    "relative_improvements = [(acc - clean_test_acc) / clean_test_acc * 100 for acc in accuracies]\n",
    "\n",
    "# 1. Accuracy Comparison Bar Chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(attack_names, accuracies, color=['blue', 'green', 'red', 'purple', 'orange'])\n",
    "plt.title('Model Accuracy under Different Attacks')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.ylim(0, 100)\n",
    "plt.savefig('accuracy_comparison.png')\n",
    "plt.show()\n",
    "\n",
    "# 2. Relative Improvement Line Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(attack_names, relative_improvements, marker='o', linestyle='--', color='orange')\n",
    "plt.title('Relative Improvement Compared to Clean Accuracy')\n",
    "plt.ylabel('Relative Improvement (%)')\n",
    "plt.grid(True)\n",
    "plt.savefig('relative_improvement.png')\n",
    "plt.show()\n",
    "\n",
    "# 3. Confusion Matrix for Clean and Gaussian Attacks\n",
    "def plot_confusion_matrix(y_true, y_pred, attack_name, filename):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {attack_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "# Assuming y_true and predictions for each scenario are available\n",
    "# plot_confusion_matrix(y_true, y_pred_clean, 'Clean', 'confusion_matrix_clean.png')\n",
    "# plot_confusion_matrix(y_true, y_pred_gaussian, 'Gaussian', 'confusion_matrix_gaussian.png')\n",
    "# plot_confusion_matrix(y_true, y_pred_mim, 'MIM', 'confusion_matrix_mim.png')  # Add for MIM\n",
    "\n",
    "# 4. Box Plot of Losses under Different Attacks\n",
    "losses_clean = no_patch_cumulative_losses\n",
    "losses_gaussian = ga_cumulative_loss\n",
    "losses_fgsm = fgsm_cumulative_loss\n",
    "losses_pgd = pgd_cumulative_loss\n",
    "losses_mim = mim_cumulative_loss\n",
    "\n",
    "# Combine data for box plot\n",
    "loss_data = [losses_clean, losses_gaussian, losses_fgsm, losses_pgd, losses_mim]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(loss_data, labels=attack_names, patch_artist=True)\n",
    "plt.title('Loss Distribution under Different Attacks')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.savefig('loss_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "# 5. Scatter Plot for Accuracy vs. Attack Strength (if applicable)\n",
    "# Assume multiple epsilon values and accuracies collected for FGSM, PGD, and MIM\n",
    "fgsm_epsilons = [0.01, 0.02, 0.03, 0.04, 0.05]\n",
    "fgsm_accuracies = [fgsm_test_acc * (1 - 0.1 * i) for i in range(len(fgsm_epsilons))]\n",
    "\n",
    "pgd_epsilons = [0.01, 0.02, 0.03, 0.04, 0.05]\n",
    "pgd_accuracies = [pgd_test_acc * (1 - 0.15 * i) for i in range(len(pgd_epsilons))]\n",
    "\n",
    "mim_epsilons = [0.01, 0.02, 0.03, 0.04, 0.05]  # Example epsilon values for MIM\n",
    "mim_accuracies = [mim_test_acc * (1 - 0.2 * i) for i in range(len(mim_epsilons))]  # Example decay for MIM\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(fgsm_epsilons, fgsm_accuracies, color='red', label='FGSM')\n",
    "plt.scatter(pgd_epsilons, pgd_accuracies, color='purple', label='PGD')\n",
    "plt.scatter(mim_epsilons, mim_accuracies, color='orange', label='MIM')  # Add MIM to scatter plot\n",
    "plt.plot(fgsm_epsilons, fgsm_accuracies, linestyle='--', color='red')\n",
    "plt.plot(pgd_epsilons, pgd_accuracies, linestyle='--', color='purple')\n",
    "plt.plot(mim_epsilons, mim_accuracies, linestyle='--', color='orange')  # Add MIM to line plot\n",
    "plt.title('Accuracy vs. Attack Strength (Epsilon)')\n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('accuracy_vs_attack_strength.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec7f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from math import pi\n",
    "\n",
    "clean_test_acc = no_patch_test_acc\n",
    "gaussian_test_acc = ga_test_acc\n",
    "fgsm_test_acc = fgsm_test_acc\n",
    "pgd_test_acc = pgd_test_acc\n",
    "mim_test_acc = mim_test_acc\n",
    "\n",
    "# Gather the results\n",
    "attack_names = ['Clean', 'Gaussian', 'FGSM', 'PGD', 'MIM']\n",
    "accuracies = [clean_test_acc, gaussian_test_acc, fgsm_test_acc, pgd_test_acc, mim_test_acc]\n",
    "\n",
    "# CDF of Losses\n",
    "losses_clean = no_patch_cumulative_losses\n",
    "losses_gaussian = ga_cumulative_loss\n",
    "losses_fgsm = fgsm_cumulative_loss\n",
    "losses_pgd = pgd_cumulative_loss\n",
    "losses_mim = mim_cumulative_loss\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.ecdfplot(losses_clean, label='Clean', color='blue')\n",
    "sns.ecdfplot(losses_gaussian, label='Gaussian', color='green')\n",
    "sns.ecdfplot(losses_fgsm, label='FGSM', color='red')\n",
    "sns.ecdfplot(losses_pgd, label='PGD', color='purple')\n",
    "sns.ecdfplot(losses_mim, label='MIM', color='orange')  # Add MIM to the CDF plot\n",
    "plt.title('CDF of Losses under Different Attacks')\n",
    "plt.xlabel('Loss')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('loss_cdf.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e978fab",
   "metadata": {},
   "source": [
    "# Composite Adverserial Attack Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d75070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attack_model(name, voxel_grids, labels, model, criterion):\n",
    "    if name == \"MIM\":\n",
    "        return mim_attack(voxel_grids, labels, model, criterion, num_iter=5)\n",
    "    elif name == \"GA\":\n",
    "        return gaussian_attack(voxel_grids)\n",
    "    elif name == \"PGD\":\n",
    "        return pgd_attack(voxel_grids, labels, model, criterion, num_iter=5)\n",
    "    elif name == \"FGSM\":\n",
    "        return fgsm_attack(voxel_grids, labels, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b2ae122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_two_attacks(model, dataloader, optimizer, criterion, device, max_batches=20, attack1=\"MIM\", attack2=\"GA\"):\n",
    "    \"\"\"\n",
    "    Train the model with two different attacks applied to the voxel grids.\n",
    "\n",
    "    Args:\n",
    "    model (nn.Module): The neural network model.\n",
    "    dataloader (DataLoader): DataLoader for the training data.\n",
    "    optimizer (Optimizer): Optimizer for training.\n",
    "    criterion (Loss): Loss function.\n",
    "    device (torch.device): Device to run the training on.\n",
    "    epsilon (float): Maximum perturbation magnitude for MIM.\n",
    "    alpha (float): Step size for each iteration in MIM.\n",
    "    num_iter (int): Number of iterations for the MIM attack.\n",
    "    decay_factor (float): Decay factor for the momentum term.\n",
    "    max_batches (int): Maximum number of batches to process in each epoch (for debugging).\n",
    "\n",
    "    Returns:\n",
    "    avg_loss (float): Average loss over the training epoch.\n",
    "    accuracy (float): Accuracy of the model on the training data.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (voxels, labels) in enumerate(dataloader):\n",
    "        if max_batches and batch_idx >= max_batches:\n",
    "            break\n",
    "        voxels = voxels.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "\n",
    "        # Apply two attack to the voxel grids\n",
    "        perturbed_voxels = get_attack_model(attack1, voxels, labels, model, criterion)\n",
    "        perturbed_voxels = get_attack_model(attack2, perturbed_voxels, labels, model, criterion)\n",
    "\n",
    "        # Forward pass with perturbed voxel grids\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(perturbed_voxels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss and accuracy\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate_with_two_attacks(model, dataloader, criterion, device, max_batches=20, attack1=\"MIM\", attack2=\"GA\"):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (voxels, labels) in enumerate(dataloader):\n",
    "        if max_batches and batch_idx >= max_batches:\n",
    "            break\n",
    "        voxels = voxels.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "\n",
    "        # Apply two attacks\n",
    "        perturbed_voxels = get_attack_model(attack1, voxels, labels, model, criterion)\n",
    "        perturbed_voxels = get_attack_model(attack2, perturbed_voxels, labels, model, criterion)\n",
    "\n",
    "        # Forward pass with perturbed voxel grids\n",
    "        outputs = model(perturbed_voxels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd336f2",
   "metadata": {},
   "source": [
    "## Apply MIM and Gaussian Attacks together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "892bf381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 is running...\n",
      "Epoch [1/10], Train Loss: 0.1406, Train Accuracy: 16.46%, Val Loss: 0.2478, Val Accuracy: 0.00%\n",
      "Epoch 2 is running...\n",
      "Epoch [2/10], Train Loss: 0.1294, Train Accuracy: 25.53%, Val Loss: 0.2291, Val Accuracy: 38.81%\n",
      "Epoch 3 is running...\n",
      "Epoch [3/10], Train Loss: 0.1315, Train Accuracy: 20.66%, Val Loss: 0.2012, Val Accuracy: 42.19%\n",
      "Epoch 4 is running...\n",
      "Epoch [4/10], Train Loss: 0.1301, Train Accuracy: 22.15%, Val Loss: 0.2169, Val Accuracy: 41.34%\n",
      "Epoch 5 is running...\n",
      "Epoch [5/10], Train Loss: 0.1236, Train Accuracy: 25.53%, Val Loss: 0.1985, Val Accuracy: 43.04%\n",
      "Epoch 6 is running...\n",
      "Epoch [6/10], Train Loss: 0.1212, Train Accuracy: 26.16%, Val Loss: 0.1870, Val Accuracy: 70.88%\n",
      "Epoch 7 is running...\n",
      "Epoch [7/10], Train Loss: 0.1186, Train Accuracy: 33.97%, Val Loss: 0.2155, Val Accuracy: 54.42%\n",
      "Epoch 8 is running...\n",
      "Epoch [8/10], Train Loss: 0.1130, Train Accuracy: 34.81%, Val Loss: 0.1932, Val Accuracy: 66.66%\n",
      "Epoch 9 is running...\n",
      "Epoch [9/10], Train Loss: 0.1115, Train Accuracy: 34.17%, Val Loss: 0.1816, Val Accuracy: 57.96%\n",
      "Epoch 10 is running...\n",
      "Epoch [10/10], Train Loss: 0.1104, Train Accuracy: 33.54%, Val Loss: 0.1909, Val Accuracy: 45.14%\n",
      "MIM and GA Attack - Test Loss: 0.1816, Test Accuracy: 70.88%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Main training loop with MIM and Gaussian attack\n",
    "mim_ga_test_loss, mim_ga_test_acc = 1, 0\n",
    "mim_ga_cumulative_loss = []\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1} is running...\")\n",
    "    train_loss, train_acc = train_model_with_two_attacks(model, train_dataloader, optimizer, criterion, device, max_batches=40, attack1=\"GA\", attack2=\"MIM\")\n",
    "    val_loss, val_acc = evaluate_with_two_attacks(model, val_dataloader, criterion, device, max_batches=20, attack1=\"GA\", attack2=\"MIM\")\n",
    "\n",
    "    mim_ga_cumulative_loss.append(val_loss)\n",
    "    mim_ga_test_loss = min(mim_ga_test_loss, val_loss)\n",
    "    mim_ga_test_acc = max(mim_ga_test_acc, val_acc)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    # Update the learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "print(f'MIM and GA Attack - Test Loss: {mim_ga_test_loss:.4f}, Test Accuracy: {mim_ga_test_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ee11ae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "mim_ga_cumulative_loss = [0.2478, 0.2291, 0.2012, 0.2169, 0.1985, 0.1870, 0.2155, 0.1932, 0.1816, 0.1909]\n",
    "mim_ga_test_loss = 0.1816\n",
    "mim_ga_test_acc = 70.88"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
