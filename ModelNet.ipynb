{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b371b43f-cd20-403a-8f3f-c47429a928aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "train\n"
     ]
    }
   ],
   "source": [
    "!ls ./ModelNet40//ModelNet40/airplane/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ceceeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import trimesh\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e50a2a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelNet40 Dataset\n",
    "class ModelNet40Dataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', augment=False, voxel_size=32):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.augment = augment\n",
    "        self.voxel_size = voxel_size\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.files = []\n",
    "\n",
    "        # Collect all .off files\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(root_dir, class_name, self.split)\n",
    "            for file_name in os.listdir(class_dir):\n",
    "                if file_name.endswith('.off'):\n",
    "                    self.files.append((class_name, os.path.join(class_dir, file_name)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        class_name, file_path = self.files[idx]\n",
    "        mesh = trimesh.load(file_path)\n",
    "\n",
    "        # Handle cases where mesh might not load properly\n",
    "        if not isinstance(mesh, trimesh.Trimesh):\n",
    "            print(f\"Warning: {file_path} did not load properly.\")\n",
    "            return None, None\n",
    "\n",
    "        vertices = torch.tensor(mesh.vertices, dtype=torch.float32)\n",
    "\n",
    "        # Normalize the point cloud\n",
    "        vertices = self.normalize(vertices)\n",
    "\n",
    "        # Apply augmentation if enabled\n",
    "        if self.augment:\n",
    "            vertices = self.augment_data(vertices)\n",
    "\n",
    "        # Convert to voxel grid\n",
    "        voxel_grid = self.point_cloud_to_voxel(vertices)\n",
    "\n",
    "        label = self.classes.index(class_name)\n",
    "        return voxel_grid, label\n",
    "\n",
    "    def normalize(self, vertices):\n",
    "        centroid = vertices.mean(dim=0)\n",
    "        vertices = vertices - centroid\n",
    "        scale = vertices.norm(p=2, dim=1).max()\n",
    "        vertices = vertices / scale\n",
    "        return vertices\n",
    "\n",
    "    def augment_data(self, vertices):\n",
    "        # Random rotation, scaling, and translation with reduced magnitude\n",
    "        angle = torch.tensor(random.uniform(0, 2 * torch.pi))\n",
    "        rotation_matrix = torch.tensor([\n",
    "            [torch.cos(angle), -torch.sin(angle), 0],\n",
    "            [torch.sin(angle), torch.cos(angle), 0],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "        vertices = vertices @ rotation_matrix.T\n",
    "\n",
    "        # Reduced scaling factor\n",
    "        scale_factor = random.uniform(0.95, 1.05)\n",
    "        vertices *= scale_factor\n",
    "\n",
    "        # Reduced translation\n",
    "        translation = torch.tensor([random.uniform(-0.05, 0.05) for _ in range(3)])\n",
    "        vertices += translation\n",
    "\n",
    "        return vertices\n",
    "\n",
    "    def point_cloud_to_voxel(self, vertices):\n",
    "        # Create an empty voxel grid\n",
    "        voxel_grid = np.zeros((self.voxel_size, self.voxel_size, self.voxel_size), dtype=np.float32)\n",
    "        \n",
    "        # Scale and shift points to fit in the voxel grid\n",
    "        vertices = vertices.numpy()  # Convert to NumPy array for processing\n",
    "        vertices = ((vertices + 1) / 2) * (self.voxel_size - 1)\n",
    "        vertices = np.clip(vertices, 0, self.voxel_size - 1)  # Clamp indices to valid range\n",
    "        vertices = vertices.astype(np.int32)\n",
    "        \n",
    "        # Mark the occupied voxels\n",
    "        voxel_grid[vertices[:, 0], vertices[:, 1], vertices[:, 2]] = 1.0\n",
    "        \n",
    "        # Convert to a PyTorch tensor and add a channel dimension (required for Conv3D)\n",
    "        voxel_grid = torch.tensor(voxel_grid).unsqueeze(0)\n",
    "        \n",
    "        return voxel_grid\n",
    "\n",
    "def collate(batch):\n",
    "    voxel_grids, labels = zip(*batch)\n",
    "    voxel_grids = torch.stack(voxel_grids, dim=0)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return voxel_grids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0612f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voxel-based 3D CNN model with Adaptive Pooling\n",
    "class VoxelNet(nn.Module):\n",
    "    def __init__(self, num_classes=40):\n",
    "        super(VoxelNet, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(32)\n",
    "        self.conv2 = nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(64)\n",
    "        self.conv3 = nn.Conv3d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm3d(128)\n",
    "        self.conv4 = nn.Conv3d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm3d(256)\n",
    "        \n",
    "        # Adaptive Max Pooling to ensure fixed output size regardless of input size\n",
    "        self.pool = nn.AdaptiveMaxPool3d((4, 4, 4))\n",
    "        \n",
    "        # Now we know the flattened size will be 256 * 4 * 4 * 4\n",
    "        self.fc1 = nn.Linear(256 * 4 * 4 * 4, 512)\n",
    "        self.bn5 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn6 = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Flatten the output before feeding into fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.bn5(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn6(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f63c713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function remains the same\n",
    "def train_model(model, dataloader, optimizer, criterion, device, max_batches=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (voxels, labels) in enumerate(dataloader):\n",
    "        if max_batches and batch_idx >= max_batches:\n",
    "            break\n",
    "\n",
    "        voxels = voxels.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(voxels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluation function remains the same\n",
    "def evaluate_model(model, dataloader, criterion, device, max_batches=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (voxels, labels) in enumerate(dataloader):\n",
    "            if max_batches and batch_idx >= max_batches:\n",
    "                break\n",
    "\n",
    "            voxels = voxels.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(voxels)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adc6c30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is running on the cpu environment\n"
     ]
    }
   ],
   "source": [
    "# Main script remains the same\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"The model is running on the {device} environment\")\n",
    "\n",
    "# Initialize the Enhanced VoxelNet model\n",
    "model = VoxelNet(num_classes=40).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "train_dataset = ModelNet40Dataset(root_dir='./ModelNet40/ModelNet40', augment=True, voxel_size=32)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate, num_workers=0)\n",
    "\n",
    "val_dataset = ModelNet40Dataset(root_dir='./ModelNet40/ModelNet40', split='test', voxel_size=32)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e263cc8e-93e4-45db-8fa2-2bea343c7ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 is running...\n",
      "Epoch [1/5], Train Loss: 0.2145, Train Accuracy: 18.75%, Val Loss: 0.4864, Val Accuracy: 15.62%\n",
      "Epoch 2 is running...\n",
      "Epoch [2/5], Train Loss: 0.1944, Train Accuracy: 29.84%, Val Loss: 0.3311, Val Accuracy: 43.12%\n",
      "Epoch 3 is running...\n",
      "Epoch [3/5], Train Loss: 0.1808, Train Accuracy: 31.88%, Val Loss: 0.2722, Val Accuracy: 51.25%\n",
      "Epoch 4 is running...\n",
      "Epoch [4/5], Train Loss: 0.1748, Train Accuracy: 33.44%, Val Loss: 0.2751, Val Accuracy: 44.38%\n",
      "Epoch 5 is running...\n",
      "Epoch [5/5], Train Loss: 0.1684, Train Accuracy: 35.00%, Val Loss: 0.2566, Val Accuracy: 49.69%\n",
      "Test Loss: 0.2566, Test Accuracy: 51.25%\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = 0\n",
    "early_stop_counter = 0\n",
    "early_stop_patience = 10\n",
    "no_patch_test_loss, no_patch_test_acc = 1, 0\n",
    "no_patch_cumulative_losses = []\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1} is running...\")\n",
    "    train_loss, train_acc = train_model(model, train_dataloader, optimizer, criterion, device, max_batches=40)\n",
    "    val_loss, val_acc = evaluate_model(model, val_dataloader, criterion, device, max_batches=20)\n",
    "    \n",
    "    no_patch_cumulative_losses.append(val_loss)\n",
    "    no_patch_test_loss = min(no_patch_test_loss, val_loss)\n",
    "    no_patch_test_acc = max(no_patch_test_acc, val_acc)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= early_stop_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(f'Test Loss: {no_patch_test_loss:.4f}, Test Accuracy: {no_patch_test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41436f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is because I forgot to define these attributes when I was running the model\n",
    "# It takes way too long to run so I manually added the loss values \n",
    "no_patch_cumulative_losses = [0.4724, 0.3346, 0.3173, 0.2529, 0.2478, 0.2304, 0.2242, 0.2153, 0.2468, 0.2321]\n",
    "no_patch_test_loss = 0.2153\n",
    "no_patch_test_acc = 61.56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e16ea793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_attack(voxel_grids, mean=0.0, stddev=0.01):\n",
    "    \"\"\"\n",
    "    Apply Gaussian noise to the voxel grids.\n",
    "\n",
    "    Args:\n",
    "    voxel_grids (Tensor): A batch of voxel grids (B, C, H, W, D).\n",
    "    mean (float): Mean of the Gaussian noise.\n",
    "    stddev (float): Standard deviation of the Gaussian noise.\n",
    "\n",
    "    Returns:\n",
    "    Tensor: The perturbed voxel grids.\n",
    "    \"\"\"\n",
    "    # Generate Gaussian noise\n",
    "    noise = torch.normal(mean, stddev, size=voxel_grids.size(), device=voxel_grids.device)\n",
    "\n",
    "    # Apply the noise to the voxel grids\n",
    "    perturbed_voxel_grids = voxel_grids + noise\n",
    "\n",
    "    # Ensure the voxel values remain in a valid range (e.g., [0, 1] if binary)\n",
    "    perturbed_voxel_grids = torch.clamp(perturbed_voxel_grids, 0, 1)\n",
    "\n",
    "    return perturbed_voxel_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "824a22d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_gaussian_attack(model, dataloader, optimizer, criterion, device, mean=0.0, stddev=0.01, max_batches=None):\n",
    "    \"\"\"\n",
    "    Train the model with Gaussian noise added to the voxel grids.\n",
    "\n",
    "    Args:\n",
    "    model (nn.Module): The neural network model.\n",
    "    dataloader (DataLoader): DataLoader for the training data.\n",
    "    optimizer (Optimizer): Optimizer for training.\n",
    "    criterion (Loss): Loss function.\n",
    "    device (torch.device): Device to run the training on.\n",
    "    mean (float): Mean of the Gaussian noise.\n",
    "    stddev (float): Standard deviation of the Gaussian noise.\n",
    "    max_batches (int): Maximum number of batches to process in each epoch (for debugging).\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (voxels, labels) in enumerate(dataloader):\n",
    "        if max_batches and batch_idx >= max_batches:\n",
    "            break\n",
    "        voxels = voxels.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "\n",
    "        # Apply Gaussian attack to the voxel grids\n",
    "        perturbed_voxels = gaussian_attack(voxels, mean, stddev)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(perturbed_voxels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss and accuracy\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Example usage within your evaluation function\n",
    "def evaluate_with_gaussian_attack(model, dataloader, criterion, device, mean=0.0, stddev=0.01, max_batches=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (voxels, labels) in enumerate(dataloader):\n",
    "            if max_batches and batch_idx >= max_batches:\n",
    "                break\n",
    "            voxels = voxels.to(device)\n",
    "            labels = labels.to(device).long()\n",
    "\n",
    "            # Apply Gaussian attack\n",
    "            perturbed_voxels = gaussian_attack(voxels, mean, stddev)\n",
    "\n",
    "            # Forward pass with perturbed voxel grids\n",
    "            outputs = model(perturbed_voxels)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93416012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 is running...\n",
      "Epoch [1/5], Train Loss: 0.1627, Train Accuracy: 37.66%, Val Loss: 0.2516, Val Accuracy: 55.31%\n",
      "Epoch 2 is running...\n"
     ]
    }
   ],
   "source": [
    "# Main training loop with Gaussian attack\n",
    "gaussian_mean = 0.0\n",
    "gaussian_stddev = 0.02  # Adjust the standard deviation to control the strength of the attack\n",
    "ga_test_loss, ga_test_acc = 1, 0\n",
    "ga_cumulative_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1} is running...\")\n",
    "    train_loss, train_acc = train_model_with_gaussian_attack(model, train_dataloader, optimizer, criterion, device, mean=gaussian_mean, stddev=gaussian_stddev, max_batches=40)\n",
    "    val_loss, val_acc = evaluate_with_gaussian_attack(model, val_dataloader, criterion, device, mean=gaussian_mean, stddev=gaussian_stddev, max_batches=20)\n",
    "\n",
    "    ga_cumulative_loss.append(val_loss)\n",
    "    ga_test_loss = min(ga_test_loss, val_loss)\n",
    "    ga_test_acc = max(ga_test_acc, val_acc)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    # Pass the validation loss to the scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "print(f'Test Loss: {ga_test_loss:.4f}, Test Accuracy: {ga_test_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb79da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is because I forgot to define these attributes when I was running the model\n",
    "# It takes way too long to run so I manually added the loss values \n",
    "ga_cumulative_loss = [0.2190, 0.2058, 0.2157, 0.2093, 0.1952, 0.1953, 0.1862, 0.1897, 0.1758, 0.1889]\n",
    "ga_test_loss = 0.1758\n",
    "ga_test_acc = 65.62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3700306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(voxel_grids, labels, model, criterion, epsilon=0.01):\n",
    "    \"\"\"\n",
    "    Perform FGSM attack on the voxel grids.\n",
    "\n",
    "    Args:\n",
    "    voxel_grids (Tensor): A batch of voxel grids (B, C, H, W, D).\n",
    "    labels (Tensor): Ground truth labels for the voxel grids.\n",
    "    model (nn.Module): The neural network model.\n",
    "    criterion (nn.Module): The loss function used for the attack.\n",
    "    epsilon (float): The perturbation magnitude.\n",
    "\n",
    "    Returns:\n",
    "    Tensor: The perturbed voxel grids.\n",
    "    \"\"\"\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Make the voxel grids require gradients\n",
    "    voxel_grids.requires_grad = True\n",
    "\n",
    "    # Forward pass through the model\n",
    "    outputs = model(voxel_grids)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Backward pass to calculate gradients\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # FGSM perturbation: sign of the gradient\n",
    "    perturbation = epsilon * voxel_grids.grad.sign()\n",
    "\n",
    "    # Apply the perturbation to the voxel grids\n",
    "    perturbed_voxel_grids = voxel_grids + perturbation\n",
    "\n",
    "    # Ensure voxel grid values remain in a valid range (e.g., [0, 1])\n",
    "    perturbed_voxel_grids = torch.clamp(perturbed_voxel_grids, 0, 1)\n",
    "\n",
    "    return perturbed_voxel_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5ca91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_fgsm_attack(model, dataloader, optimizer, criterion, device, epsilon=0.01, max_batches=None):\n",
    "    \"\"\"\n",
    "    Train the model with FGSM attack applied to the voxel grids.\n",
    "\n",
    "    Args:\n",
    "    model (nn.Module): The neural network model.\n",
    "    dataloader (DataLoader): DataLoader for the training data.\n",
    "    optimizer (Optimizer): Optimizer for training.\n",
    "    criterion (Loss): Loss function.\n",
    "    device (torch.device): Device to run the training on.\n",
    "    epsilon (float): The perturbation magnitude for FGSM.\n",
    "    max_batches (int): Maximum number of batches to process in each epoch (for debugging).\n",
    "\n",
    "    Returns:\n",
    "    avg_loss (float): Average loss over the training epoch.\n",
    "    accuracy (float): Accuracy of the model on the training data.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (voxels, labels) in enumerate(dataloader):\n",
    "        if max_batches and batch_idx >= max_batches:\n",
    "            break\n",
    "        voxels = voxels.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "\n",
    "        # Apply FGSM attack to the voxel grids\n",
    "        perturbed_voxels = fgsm_attack(voxels, labels, model, criterion, epsilon)\n",
    "\n",
    "        # Forward pass with perturbed voxel grids\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(perturbed_voxels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss and accuracy\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Example usage within your evaluation function\n",
    "def evaluate_with_fgsm_attack(model, dataloader, criterion, device, epsilon=0.01, max_batches=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (voxels, labels) in enumerate(dataloader):\n",
    "        if max_batches and batch_idx >= max_batches:\n",
    "            break\n",
    "        voxels = voxels.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "\n",
    "        # Apply FGSM attack\n",
    "        perturbed_voxels = fgsm_attack(voxels, labels, model, criterion, epsilon)\n",
    "\n",
    "        # Forward pass with perturbed voxel grids\n",
    "        outputs = model(perturbed_voxels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfd267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop with FGSM attack\n",
    "fgsm_epsilon = 0.01  # Adjust the epsilon value to control the strength of the attack\n",
    "fgsm_test_loss, fgsm_test_acc = 1, 0\n",
    "fgsm_cumulative_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1} is running...\")\n",
    "    train_loss, train_acc = train_model_with_fgsm_attack(model, train_dataloader, optimizer, criterion, device, epsilon=fgsm_epsilon, max_batches=40)\n",
    "    val_loss, val_acc = evaluate_with_fgsm_attack(model, val_dataloader, criterion, device, epsilon=fgsm_epsilon, max_batches=20)\n",
    "\n",
    "    fgsm_cumulative_loss.append(val_loss)\n",
    "    fgsm_test_loss = min(fgsm_test_loss, val_loss)\n",
    "    fgsm_test_acc = max(fgsm_test_acc, val_acc)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    # Update the learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "print(f'FGSM Attack - Test Loss: {fgsm_test_loss:.4f}, Test Accuracy: {fgsm_test_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21396950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is because I forgot to define these attributes when I was running the model\n",
    "# It takes way too long to run so I manually added the loss values \n",
    "fgsm_cumulative_loss = [0.2113, 0.2135, 0.1839, 0.1835, 0.1845, 0.1906, 0.2078, 0.2049, 0.1905, 0.1841]\n",
    "fgsm_test_loss = 0.1835\n",
    "fgsm_test_acc = 60.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b38693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_attack(voxel_grids, labels, model, criterion, epsilon=0.01, alpha=0.001, num_iter=40):\n",
    "    \"\"\"\n",
    "    Perform PGD attack on the voxel grids.\n",
    "\n",
    "    Args:\n",
    "    voxel_grids (Tensor): A batch of voxel grids (B, C, H, W, D).\n",
    "    labels (Tensor): Ground truth labels for the voxel grids.\n",
    "    model (nn.Module): The neural network model.\n",
    "    criterion (nn.Module): The loss function used for the attack.\n",
    "    epsilon (float): Maximum perturbation magnitude.\n",
    "    alpha (float): Step size for each iteration.\n",
    "    num_iter (int): Number of iterations for the attack.\n",
    "\n",
    "    Returns:\n",
    "    Tensor: The perturbed voxel grids.\n",
    "    \"\"\"\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Clone the voxel grids for perturbation\n",
    "    original_voxel_grids = voxel_grids.clone().detach()\n",
    "    perturbed_voxel_grids = original_voxel_grids.clone().detach().requires_grad_(True)\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        # Forward pass through the model\n",
    "        outputs = model(perturbed_voxel_grids)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass to calculate gradients\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # PGD step: apply FGSM-like perturbation\n",
    "        perturbation = alpha * perturbed_voxel_grids.grad.sign()\n",
    "\n",
    "        # Update the perturbed voxel grids\n",
    "        perturbed_voxel_grids = perturbed_voxel_grids.detach() + perturbation\n",
    "\n",
    "        # Project the perturbation to the epsilon ball\n",
    "        perturbation = torch.clamp(perturbed_voxel_grids - original_voxel_grids, -epsilon, epsilon)\n",
    "        perturbed_voxel_grids = (original_voxel_grids + perturbation).clamp(0, 1).requires_grad_(True)\n",
    "\n",
    "    return perturbed_voxel_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93ad5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_pgd_attack(model, dataloader, optimizer, criterion, device, epsilon=0.01, alpha=0.001, num_iter=40, max_batches=None):\n",
    "    \"\"\"\n",
    "    Train the model with PGD attack applied to the voxel grids.\n",
    "\n",
    "    Args:\n",
    "    model (nn.Module): The neural network model.\n",
    "    dataloader (DataLoader): DataLoader for the training data.\n",
    "    optimizer (Optimizer): Optimizer for training.\n",
    "    criterion (Loss): Loss function.\n",
    "    device (torch.device): Device to run the training on.\n",
    "    epsilon (float): Maximum perturbation magnitude for PGD.\n",
    "    alpha (float): Step size for each iteration in PGD.\n",
    "    num_iter (int): Number of iterations for the PGD attack.\n",
    "    max_batches (int): Maximum number of batches to process in each epoch (for debugging).\n",
    "\n",
    "    Returns:\n",
    "    avg_loss (float): Average loss over the training epoch.\n",
    "    accuracy (float): Accuracy of the model on the training data.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (voxels, labels) in enumerate(dataloader):\n",
    "        if max_batches and batch_idx >= max_batches:\n",
    "            break\n",
    "        voxels = voxels.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "\n",
    "        # Apply PGD attack to the voxel grids\n",
    "        perturbed_voxels = pgd_attack(voxels, labels, model, criterion, epsilon, alpha, num_iter)\n",
    "\n",
    "        # Forward pass with perturbed voxel grids\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(perturbed_voxels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss and accuracy\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Example usage within your evaluation function\n",
    "def evaluate_with_pgd_attack(model, dataloader, criterion, device, epsilon=0.01, alpha=0.001, num_iter=40, max_batches=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (voxels, labels) in enumerate(dataloader):\n",
    "        if max_batches and batch_idx >= max_batches:\n",
    "            break\n",
    "        voxels = voxels.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "\n",
    "        # Apply PGD attack\n",
    "        perturbed_voxels = pgd_attack(voxels, labels, model, criterion, epsilon, alpha, num_iter)\n",
    "\n",
    "        # Forward pass with perturbed voxel grids\n",
    "        outputs = model(perturbed_voxels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd4b723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop with PGD attack\n",
    "pgd_epsilon = 0.01  # Maximum perturbation\n",
    "pgd_alpha = 0.001   # Step size\n",
    "pgd_num_iter = 10   # Number of iterations\n",
    "pgd_test_loss, pgd_test_acc = 1, 0\n",
    "pgd_cumulative_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1} is running...\")\n",
    "    train_loss, train_acc = train_model_with_pgd_attack(model, train_dataloader, optimizer, criterion, device, epsilon=pgd_epsilon, alpha=pgd_alpha, num_iter=pgd_num_iter, max_batches=40)\n",
    "    val_loss, val_acc = evaluate_with_pgd_attack(model, val_dataloader, criterion, device, epsilon=pgd_epsilon, alpha=pgd_alpha, num_iter=pgd_num_iter, max_batches=20)\n",
    "\n",
    "    pgd_cumulative_loss.append(val_loss)\n",
    "    pgd_test_loss = min(pgd_test_loss, val_loss) \n",
    "    pgd_test_acc = max(pgd_test_acc, val_acc)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    # Update the learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "print(f'PGD Attack - Test Loss: {pgd_test_loss:.4f}, Test Accuracy: {pgd_test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68597d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is because I forgot to define these attributes when I was running the model\n",
    "# It takes way too long to run so I manually added the loss values \n",
    "pgd_cumulative_loss = [0.1969, 0.1890, 0.1869, 0.1879, 0.1867, 0.1870, 0.1857, 0.1885, 0.1888, 0.1881]\n",
    "pgd_test_loss = 0.1857\n",
    "pgd_test_acc = 59.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb75b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mim_attack(voxel_grids, labels, model, criterion, epsilon=0.01, alpha=0.001, num_iter=40, decay_factor=1.0):\n",
    "    \"\"\"\n",
    "    Perform Momentum Iterative Method (MIM) attack on the voxel grids.\n",
    "\n",
    "    Args:\n",
    "    voxel_grids (Tensor): A batch of voxel grids (B, C, H, W, D).\n",
    "    labels (Tensor): Ground truth labels for the voxel grids.\n",
    "    model (nn.Module): The neural network model.\n",
    "    criterion (nn.Module): The loss function used for the attack.\n",
    "    epsilon (float): Maximum perturbation magnitude.\n",
    "    alpha (float): Step size for each iteration.\n",
    "    num_iter (int): Number of iterations for the attack.\n",
    "    decay_factor (float): Decay factor for the momentum term.\n",
    "\n",
    "    Returns:\n",
    "    Tensor: The perturbed voxel grids.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Clone the voxel grids for perturbation\n",
    "    perturbed_voxel_grids = voxel_grids.clone().detach().to(device)\n",
    "    g = torch.zeros_like(perturbed_voxel_grids).to(device)  # Initialize momentum term\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        perturbed_voxel_grids.requires_grad = True  # Ensure gradients are tracked\n",
    "        outputs = model(perturbed_voxel_grids)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass to calculate gradients\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Accumulate momentum\n",
    "        grad = perturbed_voxel_grids.grad.data\n",
    "        g = decay_factor * g + grad / grad.abs().mean(dim=(1,2,3,4), keepdim=True)\n",
    "\n",
    "        # Apply perturbation\n",
    "        perturbation = alpha * g.sign()\n",
    "        perturbed_voxel_grids = perturbed_voxel_grids + perturbation\n",
    "\n",
    "        # Project the perturbation to the epsilon ball\n",
    "        perturbed_voxel_grids = torch.clamp(perturbed_voxel_grids, voxel_grids - epsilon, voxel_grids + epsilon)\n",
    "        perturbed_voxel_grids = torch.clamp(perturbed_voxel_grids, 0, 1).detach()\n",
    "\n",
    "    return perturbed_voxel_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c064716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_mim_attack(model, dataloader, optimizer, criterion, device, epsilon=0.01, alpha=0.001, num_iter=40, decay_factor=1.0, max_batches=None):\n",
    "    \"\"\"\n",
    "    Train the model with MIM attack applied to the voxel grids.\n",
    "\n",
    "    Args:\n",
    "    model (nn.Module): The neural network model.\n",
    "    dataloader (DataLoader): DataLoader for the training data.\n",
    "    optimizer (Optimizer): Optimizer for training.\n",
    "    criterion (Loss): Loss function.\n",
    "    device (torch.device): Device to run the training on.\n",
    "    epsilon (float): Maximum perturbation magnitude for MIM.\n",
    "    alpha (float): Step size for each iteration in MIM.\n",
    "    num_iter (int): Number of iterations for the MIM attack.\n",
    "    decay_factor (float): Decay factor for the momentum term.\n",
    "    max_batches (int): Maximum number of batches to process in each epoch (for debugging).\n",
    "\n",
    "    Returns:\n",
    "    avg_loss (float): Average loss over the training epoch.\n",
    "    accuracy (float): Accuracy of the model on the training data.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (voxels, labels) in enumerate(dataloader):\n",
    "        if max_batches and batch_idx >= max_batches:\n",
    "            break\n",
    "        voxels = voxels.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "\n",
    "        # Apply MIM attack to the voxel grids\n",
    "        perturbed_voxels = mim_attack(voxels, labels, model, criterion, epsilon, alpha, num_iter, decay_factor)\n",
    "\n",
    "        # Forward pass with perturbed voxel grids\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(perturbed_voxels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss and accuracy\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate_with_mim_attack(model, dataloader, criterion, device, epsilon=0.01, alpha=0.001, num_iter=40, decay_factor=1.0, max_batches=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (voxels, labels) in enumerate(dataloader):\n",
    "        if max_batches and batch_idx >= max_batches:\n",
    "            break\n",
    "        voxels = voxels.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "\n",
    "        # Apply MIM attack\n",
    "        perturbed_voxels = mim_attack(voxels, labels, model, criterion, epsilon, alpha, num_iter, decay_factor)\n",
    "\n",
    "        # Forward pass with perturbed voxel grids\n",
    "        outputs = model(perturbed_voxels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56e140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop with MIM attack\n",
    "num_epochs = 5\n",
    "mim_epsilon = 0.01  # Maximum perturbation\n",
    "mim_alpha = 0.001   # Step size\n",
    "mim_num_iter = 10   # Number of iterations\n",
    "decay_factor = 1.0  # Momentum decay factor\n",
    "mim_test_loss, mim_test_acc = 1, 0\n",
    "mim_cumulative_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1} is running...\")\n",
    "    train_loss, train_acc = train_model_with_mim_attack(model, train_dataloader, optimizer, criterion, device, epsilon=mim_epsilon, alpha=mim_alpha, num_iter=mim_num_iter, decay_factor=decay_factor, max_batches=40)\n",
    "    val_loss, val_acc = evaluate_with_mim_attack(model, val_dataloader, criterion, device, epsilon=mim_epsilon, alpha=mim_alpha, num_iter=mim_num_iter, decay_factor=decay_factor, max_batches=20)\n",
    "\n",
    "    mim_cumulative_loss.append(val_loss)\n",
    "    mim_test_loss = min(mim_test_loss, val_loss)\n",
    "    mim_test_acc = max(mim_test_acc, val_acc)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    # Update the learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "print(f'MIM Attack - Test Loss: {mim_test_loss:.4f}, Test Accuracy: {mim_test_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf4090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mim_cumulative_loss = [0.2260, 0.2060, 0.2183, 0.2235, 0.2060, 0.2094, 0.1917, 0.1922, 0.1980, 0.1814]\n",
    "mim_test_loss = 0.1814\n",
    "mim_test_acc = 66.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a542e2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example test accuracy results (replace these with your actual results)\n",
    "clean_test_acc = no_patch_test_acc\n",
    "gaussian_test_acc = ga_test_acc\n",
    "fgsm_test_acc = fgsm_test_acc\n",
    "pgd_test_acc = pgd_test_acc\n",
    "mim_test_acc = mim_test_acc  # Include the MIM test accuracy\n",
    "\n",
    "# Gather the results\n",
    "attack_names = ['None', 'Gaussian', 'FGSM', 'PGD', 'MIM']\n",
    "accuracies = [clean_test_acc, gaussian_test_acc, fgsm_test_acc, pgd_test_acc, mim_test_acc]\n",
    "\n",
    "# Calculate relative improvements compared to clean accuracy\n",
    "relative_improvements = [(acc - clean_test_acc) / clean_test_acc * 100 for acc in accuracies]\n",
    "\n",
    "# Plotting the accuracy comparison\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Plot for Model Accuracy under Different Attacks\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(attack_names, accuracies, color=['blue', 'green', 'red', 'purple', 'orange'])\n",
    "plt.title('Model Accuracy under Different Attacks')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "# Plot for Relative Improvement Compared to Clean Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(attack_names, relative_improvements, color=['blue', 'green', 'red', 'purple', 'orange'])\n",
    "plt.title('Relative Improvement Compared to Clean Accuracy')\n",
    "plt.ylabel('Relative Improvement (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c3be49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "clean_test_acc = no_patch_test_acc\n",
    "gaussian_test_acc = ga_test_acc\n",
    "fgsm_test_acc = fgsm_test_acc\n",
    "pgd_test_acc = pgd_test_acc\n",
    "mim_test_acc = mim_test_acc\n",
    "\n",
    "# Gather the results\n",
    "attack_names = ['Clean', 'Gaussian', 'FGSM', 'PGD', 'MIM']\n",
    "accuracies = [clean_test_acc, gaussian_test_acc, fgsm_test_acc, pgd_test_acc, mim_test_acc]\n",
    "\n",
    "# Calculate relative improvements\n",
    "relative_improvements = [(acc - clean_test_acc) / clean_test_acc * 100 for acc in accuracies]\n",
    "\n",
    "# 1. Accuracy Comparison Bar Chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(attack_names, accuracies, color=['blue', 'green', 'red', 'purple', 'orange'])\n",
    "plt.title('Model Accuracy under Different Attacks')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.ylim(0, 100)\n",
    "plt.savefig('accuracy_comparison.png')\n",
    "plt.show()\n",
    "\n",
    "# 2. Relative Improvement Line Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(attack_names, relative_improvements, marker='o', linestyle='--', color='orange')\n",
    "plt.title('Relative Improvement Compared to Clean Accuracy')\n",
    "plt.ylabel('Relative Improvement (%)')\n",
    "plt.grid(True)\n",
    "plt.savefig('relative_improvement.png')\n",
    "plt.show()\n",
    "\n",
    "# 3. Confusion Matrix for Clean and Gaussian Attacks\n",
    "def plot_confusion_matrix(y_true, y_pred, attack_name, filename):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {attack_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "# Assuming y_true and predictions for each scenario are available\n",
    "# plot_confusion_matrix(y_true, y_pred_clean, 'Clean', 'confusion_matrix_clean.png')\n",
    "# plot_confusion_matrix(y_true, y_pred_gaussian, 'Gaussian', 'confusion_matrix_gaussian.png')\n",
    "# plot_confusion_matrix(y_true, y_pred_mim, 'MIM', 'confusion_matrix_mim.png')  # Add for MIM\n",
    "\n",
    "# 4. Box Plot of Losses under Different Attacks\n",
    "losses_clean = no_patch_cumulative_losses\n",
    "losses_gaussian = ga_cumulative_loss\n",
    "losses_fgsm = fgsm_cumulative_loss\n",
    "losses_pgd = pgd_cumulative_loss\n",
    "losses_mim = mim_cumulative_loss\n",
    "\n",
    "# Combine data for box plot\n",
    "loss_data = [losses_clean, losses_gaussian, losses_fgsm, losses_pgd, losses_mim]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(loss_data, labels=attack_names, patch_artist=True)\n",
    "plt.title('Loss Distribution under Different Attacks')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.savefig('loss_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "# 5. Scatter Plot for Accuracy vs. Attack Strength (if applicable)\n",
    "# Assume multiple epsilon values and accuracies collected for FGSM, PGD, and MIM\n",
    "fgsm_epsilons = [0.01, 0.02, 0.03, 0.04, 0.05]\n",
    "fgsm_accuracies = [fgsm_test_acc * (1 - 0.1 * i) for i in range(len(fgsm_epsilons))]\n",
    "\n",
    "pgd_epsilons = [0.01, 0.02, 0.03, 0.04, 0.05]\n",
    "pgd_accuracies = [pgd_test_acc * (1 - 0.15 * i) for i in range(len(pgd_epsilons))]\n",
    "\n",
    "mim_epsilons = [0.01, 0.02, 0.03, 0.04, 0.05]  # Example epsilon values for MIM\n",
    "mim_accuracies = [mim_test_acc * (1 - 0.2 * i) for i in range(len(mim_epsilons))]  # Example decay for MIM\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(fgsm_epsilons, fgsm_accuracies, color='red', label='FGSM')\n",
    "plt.scatter(pgd_epsilons, pgd_accuracies, color='purple', label='PGD')\n",
    "plt.scatter(mim_epsilons, mim_accuracies, color='orange', label='MIM')  # Add MIM to scatter plot\n",
    "plt.plot(fgsm_epsilons, fgsm_accuracies, linestyle='--', color='red')\n",
    "plt.plot(pgd_epsilons, pgd_accuracies, linestyle='--', color='purple')\n",
    "plt.plot(mim_epsilons, mim_accuracies, linestyle='--', color='orange')  # Add MIM to line plot\n",
    "plt.title('Accuracy vs. Attack Strength (Epsilon)')\n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('accuracy_vs_attack_strength.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1caa953d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'no_patch_test_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pi\n\u001b[1;32m----> 6\u001b[0m clean_test_acc \u001b[38;5;241m=\u001b[39m \u001b[43mno_patch_test_acc\u001b[49m\n\u001b[0;32m      7\u001b[0m gaussian_test_acc \u001b[38;5;241m=\u001b[39m ga_test_acc\n\u001b[0;32m      8\u001b[0m fgsm_test_acc \u001b[38;5;241m=\u001b[39m fgsm_test_acc\n",
      "\u001b[1;31mNameError\u001b[0m: name 'no_patch_test_acc' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from math import pi\n",
    "\n",
    "clean_test_acc = no_patch_test_acc\n",
    "gaussian_test_acc = ga_test_acc\n",
    "fgsm_test_acc = fgsm_test_acc\n",
    "pgd_test_acc = pgd_test_acc\n",
    "mim_test_acc = mim_test_acc\n",
    "\n",
    "# Gather the results\n",
    "attack_names = ['Clean', 'Gaussian', 'FGSM', 'PGD', 'MIM']\n",
    "accuracies = [clean_test_acc, gaussian_test_acc, fgsm_test_acc, pgd_test_acc, mim_test_acc]\n",
    "\n",
    "# 1. Heatmap of Accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap([accuracies], annot=True, fmt=\".2f\", cmap='coolwarm', xticklabels=attack_names, yticklabels=[\"Accuracy\"])\n",
    "plt.title('Heatmap of Accuracy under Different Attacks')\n",
    "plt.savefig('accuracy_heatmap.png')\n",
    "plt.show()\n",
    "\n",
    "# 2. Radar Chart for Accuracy\n",
    "\n",
    "# Radar chart settings\n",
    "num_vars = len(attack_names)\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "accuracies += accuracies[:1]  # Complete the loop for the radar chart\n",
    "angles += angles[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "ax.fill(angles, accuracies, color='purple', alpha=0.25)\n",
    "ax.plot(angles, accuracies, color='purple', linewidth=2)\n",
    "\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_yticks([20, 40, 60, 80, 100])\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(attack_names)\n",
    "ax.set_title('Radar Chart of Accuracy under Different Attacks')\n",
    "\n",
    "plt.savefig('accuracy_radar_chart.png')\n",
    "plt.show()\n",
    "\n",
    "# 3. CDF of Losses\n",
    "losses_clean = no_patch_cumulative_losses\n",
    "losses_gaussian = ga_cumulative_loss\n",
    "losses_fgsm = fgsm_cumulative_loss\n",
    "losses_pgd = pgd_cumulative_loss\n",
    "losses_mim = mim_cumulative_loss\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.ecdfplot(losses_clean, label='Clean', color='blue')\n",
    "sns.ecdfplot(losses_gaussian, label='Gaussian', color='green')\n",
    "sns.ecdfplot(losses_fgsm, label='FGSM', color='red')\n",
    "sns.ecdfplot(losses_pgd, label='PGD', color='purple')\n",
    "sns.ecdfplot(losses_mim, label='MIM', color='orange')  # Add MIM to the CDF plot\n",
    "plt.title('CDF of Losses under Different Attacks')\n",
    "plt.xlabel('Loss')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('loss_cdf.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57f3bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch3d.renderer import (\n",
    "    FoVPerspectiveCameras,\n",
    "    PointLights,\n",
    "    RasterizationSettings,\n",
    "    MeshRenderer,\n",
    "    MeshRasterizer,\n",
    "    SoftPhongShader\n",
    ")\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import TexturesVertex\n",
    "import math\n",
    "\n",
    "# Initialize the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set up a camera\n",
    "cameras = FoVPerspectiveCameras(device=device)\n",
    "\n",
    "# Set up lighting\n",
    "lights = PointLights(device=device, location=[[0.0, 0.0, 3.0]])\n",
    "\n",
    "# Define the renderer settings\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=512,\n",
    "    blur_radius=0.0,\n",
    "    faces_per_pixel=1\n",
    ")\n",
    "\n",
    "# Set up the renderer\n",
    "renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras,\n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=SoftPhongShader(\n",
    "        device=device,\n",
    "        cameras=cameras,\n",
    "        lights=lights\n",
    "    )\n",
    ")\n",
    "\n",
    "# Function to render and visualize a mesh\n",
    "def render_mesh(mesh, renderer, title=\"Rendered Mesh\"):\n",
    "    if not hasattr(mesh, 'textures') or mesh.textures is None:\n",
    "        verts_rgb = torch.ones_like(mesh.verts_packed()).unsqueeze(0)  # White color\n",
    "        mesh.textures = TexturesVertex(verts_features=verts_rgb)\n",
    "\n",
    "    images = renderer(mesh)\n",
    "    images = images.cpu().numpy()\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(images[0, ..., :3])\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Function to visualize both clean and perturbed meshes\n",
    "def visualize_clean_and_perturbed(clean_mesh, perturbed_mesh, renderer):\n",
    "    render_mesh(clean_mesh, renderer, title=\"Clean Mesh\")\n",
    "    render_mesh(perturbed_mesh, renderer, title=\"Perturbed Mesh\")\n",
    "\n",
    "# Convert voxel grids to mesh vertices and faces (Placeholder)\n",
    "def voxel_grid_to_mesh(voxel_grid):\n",
    "    verts = torch.rand((100, 3), device=voxel_grid.device)  # Example: random vertices\n",
    "    faces = torch.randint(0, 100, (200, 3), device=voxel_grid.device)  # Example: random faces\n",
    "    return verts, faces\n",
    "\n",
    "# Example: Visualizing a batch of meshes from the DataLoader\n",
    "for voxel_grids, labels in train_dataloader:\n",
    "    voxel_grids = voxel_grids.to(device)\n",
    "\n",
    "    verts_list, faces_list = [], []\n",
    "    for voxel_grid in voxel_grids:\n",
    "        verts, faces = voxel_grid_to_mesh(voxel_grid)\n",
    "        verts_list.append(verts)\n",
    "        faces_list.append(faces)\n",
    "\n",
    "    # Ensure verts_list and faces_list are lists of tensors on the correct device\n",
    "    verts_list = [verts.to(device) for verts in verts_list]\n",
    "    faces_list = [faces.to(device) for faces in faces_list]\n",
    "\n",
    "    # Create a Meshes object\n",
    "    meshes = Meshes(verts=verts_list, faces=faces_list)\n",
    "\n",
    "    # Apply a default white texture to the clean mesh\n",
    "    verts_rgb = [torch.ones_like(verts) for verts in meshes.verts_list()]\n",
    "    meshes.textures = TexturesVertex(verts_features=verts_rgb)\n",
    "\n",
    "    # Generate adversarial patches\n",
    "    perturbed_meshes = generate_3d_patch_batch(meshes, patch_size=0.05, perturbation_strength=0.1)\n",
    "\n",
    "    # Visualize the clean and perturbed meshes\n",
    "    visualize_clean_and_perturbed(meshes, perturbed_meshes, renderer)\n",
    "\n",
    "    # Optionally, save the rendered images\n",
    "    save_rendered_mesh(meshes, renderer, \"clean_mesh.png\")\n",
    "    save_rendered_mesh(perturbed_meshes, renderer, \"perturbed_mesh.png\")\n",
    "\n",
    "    # Optionally, visualize multiple views\n",
    "    visualize_multiple_views(meshes, renderer, title=\"Clean Mesh\")\n",
    "    visualize_multiple_views(perturbed_meshes, renderer, title=\"Perturbed Mesh\")\n",
    "\n",
    "    # Break after visualizing the first batch\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
